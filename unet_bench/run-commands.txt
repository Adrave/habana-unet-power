Commands:

(Within Habana)
$PYTHON unet.py --hpu --use_lazy_mode --epochs 50 is the current default.

This text file is to stage various versions runs, so the commands don't need to be written on the remote. Remove the ticks, of course.
It's also pretty messy, obviously.

basic run:

$PYTHON unet.py --hpu --use_lazy_mode --run-name mem-print-1 --epochs 5 --cache-path kaggle_cache --weights-file m-p-1.pt

basic distributed run:
mpirun -n 2 --rank-by core $PYTHON unet.py --hpu --use_lazy_mode --distributed --run-name 64-test-workers-1 --epochs 5 --image-size 64 --batch-size 64 --cache-path kaggle_cache --weights-file 64-t-w-1.pt --world-size 2 --num-workers 1
mpirun -n 2 --rank-by core $PYTHON unet.py --hpu --use_lazy_mode --distributed --run-name 64-test-workers-2 --epochs 5 --image-size 64 --batch-size 64 --cache-path kaggle_cache --weights-file 64-t-w-2.pt --world-size 2 --num-workers 2
mpirun -n 2 --rank-by core $PYTHON unet.py --hpu --use_lazy_mode --distributed --run-name 64-test-workers-6 --epochs 5 --image-size 64 --batch-size 64 --cache-path kaggle_cache --weights-file 64-t-w-6.pt --world-size 2 --num-workers 6
mpirun -n 2 --rank-by core $PYTHON unet.py --hpu --use_lazy_mode --distributed --run-name 64-test-workers-10 --epochs 5 --image-size 64 --batch-size 64 --cache-path kaggle_cache --weights-file 64-t-cw-10.pt --world-size 2 --num-workers 10
^ Resulted in higher workers = longer
mpirun -n 2 --rank-by core $PYTHON unet.py --hpu --use_lazy_mode --distributed --run-name 64-test-workers-4-4 --epochs 5 --image-size 64 --batch-size 64 --cache-path kaggle_cache --weights-file 64-t-w-4.pt --world-size 2 --num-workers 4
mpirun -n 4 --rank-by core $PYTHON unet.py --hpu --use_lazy_mode --distributed --run-name 64-test-workers-4 --epochs 5 --image-size 64 --batch-size 64 --cache-path kaggle_cache --weights-file 64-t-n-4.pt --world-size 4 --num-workers 4

batch-size change:
mpirun -n 2 --bind-to core --rank-by core --allow-run-as-root $PYTHON unet.py --hpu --use_lazy_mode --distributed --run-name 64-test-b-128 --epochs 5 --image-size 64 --batch-size 128 --cache-path kaggle_cache --weights-file 64-t-b-128.pt --world-size 2
mpirun -n 2 --bind-to core --rank-by core --allow-run-as-root $PYTHON unet.py --hpu --use_lazy_mode --distributed --run-name 64-test-b-16 --epochs 5 --image-size 64 --batch-size 16 --cache-path kaggle_cache --weights-file 64-t-b-16.pt --world-size 2

single-comparisons:
$PYTHON unet.py --hpu --use_lazy_mode --run-name 256-test-workers-1-128 --epochs 5 --image-size 256 --batch-size 128 --cache-path kaggle_cache --weights-file 256-t-w-1-128.pt
$PYTHON unet.py --hpu --use_lazy_mode --run-name 256-test-workers-2-128 --epochs 5 --image-size 256 --batch-size 128 --cache-path kaggle_cache --weights-file 256-t-w-2-128.pt --num-workers 2


up in image-size:
$PYTHON unet.py --hpu --use_lazy_mode --run-name 256-batch-64 --epochs 5 --image-size 256 --cache-path kaggle_256_cache --weights-file 256-b-64.pt
mpirun -n 2 --rank-by core $PYTHON unet.py --hpu --use_lazy_mode --distributed --run-name 256-test-cards-2 --epochs 5 --image-size 256 --batch-size 64 --cache-path kaggle_256_cache --weights-file 256-t-c-2.pt --world-size 2

theta and habana distrib runs:

time mpirun -n 2 --rank-by core python3 unet_local.py --distributed --run-name theta-dist-test --epochs 5 --weights-file t-d-t.pt --world-size 2

python3 unet_local.py --run-name theta-dist-test-2 --epochs 5 --weights-file t-d-t-2.pt

mpirun -n 2 --rank-by core python3 unet_local.py --distributed --run-name theta-test-workers-1 --epochs 5 --weights-file t-t-w-1.pt --world-size 2 --num-workers 1
mpirun -n 2 --rank-by core python3 unet_local.py --distributed --run-name theta-test-workers-2 --epochs 5 --weights-file t-t-w-2.pt --world-size 2 --num-workers 2
mpirun -n 2 --rank-by core python3 unet_local.py --distributed --run-name theta-test-workers-6 --epochs 5 --weights-file t-t-w-6.pt --world-size 2 --num-workers 6
mpirun -n 2 --rank-by core python3 unet_local.py --distributed --run-name theta-test-workers-10 --epochs 5 --weights-file t-t-w-10.pt --world-size 2 --num-workers 10

mpirun -n 2 --rank-by core python3 unet_local.py --distributed --run-name theta-test-workers-2-4 --epochs 5 --weights-file t-t-w-2-4.pt --world-size 2 --num-workers 4
mpirun -n 4 --rank-by core python3 unet_local.py --distributed --run-name theta-test-workers-4-4 --epochs 5 --weights-file t-t-w-4-4.pt --world-size 4 --num-workers 4
mpirun -n 8 --rank-by core python3 unet_local.py --distributed --run-name theta-test-workers-8-8 --epochs 5 --weights-file t-t-w-8-8.pt --world-size 8 --num-workers 8

python3 unet_local.py --run-name theta-test-worker-1-256 --epochs 5 --image-size 256 --cache-path kaggle_256_cache --weights-file t-t-w-1-256.pt
mpirun -n 2 --rank-by core python3 unet_local.py --distributed --run-name theta-test-worker-2-256 --epochs 5 --image-size 256 --cache-path kaggle_256_cache --weights-file t-t-w-2-256.pt --world-size 2 --num-workers 2
mpirun -n 4 --rank-by core python3 unet_local.py --distributed --run-name theta-test-worker-4-256 --epochs 5 --image-size 256 --cache-path kaggle_256_cache --weights-file t-t-w-4-256.pt --world-size 4 --num-workers 4
mpirun -n 8 --rank-by core python3 unet_local.py --distributed --run-name theta-test-worker-8-256 --epochs 5 --image-size 256 --cache-path kaggle_256_cache --weights-file t-t-w-8-256.pt --world-size 8 --num-workers 8

python3 unet_local.py --run-name theta-test-worker-2-duped --epochs 5 --cache-path kaggle_duped_cache --weights-file t-t-w-1-1.pt
mpirun -n 2 --rank-by core python3 unet_local.py --distributed --run-name theta-test-worker-2-duped --epochs 5 --cache-path kaggle_duped_cache --weights-file t-t-w-2-d.pt --world-size 2 --num-workers 2
mpirun -n 4 --rank-by core python3 unet_local.py --distributed --run-name theta-test-worker-4-duped --epochs 5 --cache-path kaggle_duped_cache --weights-file t-t-w-4-d.pt --world-size 4 --num-workers 4
mpirun -n 8 --rank-by core python3 unet_local.py --distributed --run-name theta-test-worker-8-duped --epochs 5 --cache-path kaggle_duped_cache --weights-file t-t-w-8-d.pt --world-size 8 --num-workers 8

$PYTHON unet.py --hpu --use_lazy_mode --run-name habana-worker-1-duped --epochs 5 --cache-path kaggle_duped_cache --weights-file h-w-1-d.pt
mpirun -n 2 --rank-by core $PYTHON unet.py --hpu --use_lazy_mode --distributed --run-name habana-worker-2-duped --epochs 5 --cache-path kaggle_duped_cache --weights-file h-w-2-d.pt --world-size 2 --num-workers 2
mpirun -n 4 --rank-by core $PYTHON unet.py --hpu --use_lazy_mode --distributed --run-name habana-worker-4-duped --epochs 5 --cache-path kaggle_duped_cache --weights-file h-w-4-d.pt --world-size 4 --num-workers 4
^ crashed, so didn't execute below.
mpirun -n 8 --rank-by core $PYTHON unet.py --hpu --use_lazy_mode --distributed --run-name habana-worker-8-duped --epochs 5 --cache-path kaggle_duped_cache --weights-file h-w-8-d.pt --world-size 8 --num-workers 8